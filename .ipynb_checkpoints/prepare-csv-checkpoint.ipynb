{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133d32f3-e741-4380-846b-50e3b505c1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anaykulkarni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anaykulkarni/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from reviewsdataset import loadBatchListwise, getReviews\n",
    "from itemspecificity import getItemSpecificity\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46348b2f-ca76-4551-9d6a-c0f927ffded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = getReviews()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceab2004-376e-44ba-9b10-298a750a550c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_freq, inv_item_freq = getItemSpecificity(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6eabc6-f024-4233-9f4c-d262e11dfec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "sw = stopwords.words('english')\n",
    "sp = string.punctuation\n",
    "\n",
    "def get_item_specificity(word, bookid):\n",
    "    return doc_freq[bookid][word] * inv_item_freq[word]\n",
    "\n",
    "def get_score_by_sentence_max(row):\n",
    "    text, bookid = row['sentences'], row['bookid']\n",
    "    words = [word for word in text.split() if word.lower() not in sw]\n",
    "    cleaned_words = [''.join([c for c in word if c not in sp]) for word in words if len(word)>1]\n",
    "    if len(cleaned_words) < 1:\n",
    "        return 0\n",
    "    score = np.max([get_item_specificity(w, bookid) for w in cleaned_words])\n",
    "    return score\n",
    "\n",
    "def get_score_by_sentence_mean(row):\n",
    "    text, bookid = row['sentences'], row['bookid']\n",
    "    words = [word for word in text.split() if word.lower() not in sw]\n",
    "    cleaned_words = [''.join([c for c in word if c not in sp]) for word in words if len(word)>1]\n",
    "    if len(cleaned_words) < 1:\n",
    "        return 0\n",
    "    score = np.mean([get_item_specificity(w, bookid) for w in cleaned_words])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "166078a7-ca29-4eed-ad79-8887643106bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.concat([pd.DataFrame(loadBatchListwise(r, i)) for i, r in enumerate(reviews[:700])]).reset_index(drop=True)\n",
    "valdf = pd.concat([pd.DataFrame(loadBatchListwise(r, i)) for i, r in enumerate(reviews[700:900])]).reset_index(drop=True)\n",
    "testdf = pd.concat([pd.DataFrame(loadBatchListwise(r, i)) for i, r in enumerate(reviews[900:1000])]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ed1de6c-be3d-4365-8110-2d803c6ca9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf['item_spec_score_max'] = traindf.apply(get_score_by_sentence_max, axis=1)\n",
    "valdf['item_spec_score_max'] = valdf.apply(get_score_by_sentence_max, axis=1)\n",
    "testdf['item_spec_score_max'] = testdf.apply(get_score_by_sentence_max, axis=1)\n",
    "traindf['item_spec_score_mean'] = traindf.apply(get_score_by_sentence_mean, axis=1)\n",
    "valdf['item_spec_score_mean'] = valdf.apply(get_score_by_sentence_mean, axis=1)\n",
    "testdf['item_spec_score_mean'] = testdf.apply(get_score_by_sentence_mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583e8e2a-b2a6-4b0c-9502-3efaf7837125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english',  # Remove common stopwords\n",
    "                                   ngram_range=(1, 1),\n",
    "                                   norm='l2',\n",
    "                                   lowercase=True,\n",
    "                                   use_idf=True,\n",
    "                                   smooth_idf=True)\n",
    "# traindf['tfidfscore'] = np.array(tfidf_vectorizer.fit_transform(traindf['sentences']).sum(axis=1))\n",
    "# valdf['tfidfscore'] = np.array(tfidf_vectorizer.transform(valdf['sentences']).sum(axis=1))\n",
    "# testdf['tfidfscore'] = np.array(tfidf_vectorizer.transform(testdf['sentences']).sum(axis=1))\n",
    "\n",
    "# Train data: Compute max TF-IDF score for each sentence\n",
    "traindf['tfidfscore'] = np.array(\n",
    "    tfidf_vectorizer.fit_transform(traindf['sentences']).max(axis=1).toarray()\n",
    ").flatten()\n",
    "\n",
    "# Validation data: Compute max TF-IDF score for each sentence\n",
    "valdf['tfidfscore'] = np.array(\n",
    "    tfidf_vectorizer.transform(valdf['sentences']).max(axis=1).toarray()\n",
    ").flatten()\n",
    "\n",
    "# Test data: Compute max TF-IDF score for each sentence\n",
    "testdf['tfidfscore'] = np.array(\n",
    "    tfidf_vectorizer.transform(testdf['sentences']).max(axis=1).toarray()\n",
    ").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d9e2c5f-07a8-4ee3-9517-d86bfd76f7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "traindf['positions'] = scaler.fit_transform(traindf[['positions']])\n",
    "valdf['positions'] = scaler.fit_transform(valdf[['positions']])\n",
    "testdf['positions'] = scaler.fit_transform(testdf[['positions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93dcbb1b-8ba6-4ecd-a50e-51700b34239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def get_combined_embedding(row):\n",
    "    # Combine the 'sentence' and 'context' columns\n",
    "    # combined_text = f\"{row['sentences']}\" # [SEP] {row['contexts']}\"\n",
    "    # Tokenize the combined text\n",
    "    inputs = tokenizer(row['sentences'], return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    # Pass through the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the pooler_output (CLS token embedding) as the sentence embedding\n",
    "    # embedding = outputs.pooler_output.squeeze(0)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0)\n",
    "    return embedding.numpy()\n",
    "\n",
    "# Apply the function to calculate embeddings\n",
    "traindf['combined_embedding'] = traindf.apply(get_combined_embedding, axis=1)\n",
    "valdf['combined_embedding'] = valdf.apply(get_combined_embedding, axis=1)\n",
    "testdf['combined_embedding'] = testdf.apply(get_combined_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d92ad9d-8db0-4912-b2e4-b64fc585b44b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_embeddings = np.array(traindf['combined_embedding'].to_list())\n",
    "val_embeddings = np.array(valdf['combined_embedding'].to_list())\n",
    "test_embeddings = np.array(testdf['combined_embedding'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab93b243-9322-4807-af1c-f981dcab799f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14295, 768)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "386cd7ac-343c-4906-b03c-e60966450d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensions (e.g., from 768 to 128)\n",
    "pca = PCA(n_components=128)\n",
    "reduced_train_embeddings = pca.fit_transform(train_embeddings)\n",
    "reduced_val_embeddings = pca.transform(val_embeddings)\n",
    "reduced_test_embeddings = pca.transform(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7accdbe-ed46-40d5-b7d4-63e5d6e323aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14295, 128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60c5c7f1-00ac-40e2-a261-0e2ce03743ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of reduced embeddings to a DataFrame\n",
    "embeddings_df_train = pd.DataFrame(reduced_train_embeddings, index=traindf.index)\n",
    "embeddings_df_val = pd.DataFrame(reduced_val_embeddings, index=valdf.index)\n",
    "embeddings_df_test = pd.DataFrame(reduced_test_embeddings, index=testdf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ff58810-d1f7-4026-a476-de35289576bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the reduced embeddings with the original DataFrame\n",
    "traindf = pd.concat([traindf, embeddings_df_train], axis=1)\n",
    "valdf = pd.concat([valdf, embeddings_df_val], axis=1)\n",
    "testdf = pd.concat([testdf, embeddings_df_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d5f2626-49a5-4f52-9471-23e2649bd658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>contexts</th>\n",
       "      <th>labels</th>\n",
       "      <th>positions</th>\n",
       "      <th>reviewid</th>\n",
       "      <th>bookid</th>\n",
       "      <th>item_spec_score_max</th>\n",
       "      <th>item_spec_score_mean</th>\n",
       "      <th>tfidfscore</th>\n",
       "      <th>combined_embedding</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What a fun series.</td>\n",
       "      <td>Dust</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>17855756</td>\n",
       "      <td>0.015290</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>0.785133</td>\n",
       "      <td>[0.13568804, -0.42419568, 0.22611226, 0.193501...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435559</td>\n",
       "      <td>-0.028487</td>\n",
       "      <td>0.224099</td>\n",
       "      <td>0.136431</td>\n",
       "      <td>0.306240</td>\n",
       "      <td>0.047697</td>\n",
       "      <td>0.344808</td>\n",
       "      <td>0.247321</td>\n",
       "      <td>0.494328</td>\n",
       "      <td>-0.217180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I loved Wool, and Dust and Shift both gave us ...</td>\n",
       "      <td>Dust</td>\n",
       "      <td>0</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0</td>\n",
       "      <td>17855756</td>\n",
       "      <td>0.050604</td>\n",
       "      <td>0.027685</td>\n",
       "      <td>0.417833</td>\n",
       "      <td>[0.27358928, 0.051419273, 0.13299458, 0.160809...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457130</td>\n",
       "      <td>-0.071240</td>\n",
       "      <td>0.153160</td>\n",
       "      <td>-0.042372</td>\n",
       "      <td>0.042692</td>\n",
       "      <td>0.019526</td>\n",
       "      <td>0.195423</td>\n",
       "      <td>0.173676</td>\n",
       "      <td>0.141578</td>\n",
       "      <td>-0.039212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I think the first book was by far the best, bu...</td>\n",
       "      <td>Dust</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0</td>\n",
       "      <td>17855756</td>\n",
       "      <td>0.050604</td>\n",
       "      <td>0.026657</td>\n",
       "      <td>0.484192</td>\n",
       "      <td>[-0.07830019, -0.3241384, 0.26186368, -0.03014...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.182080</td>\n",
       "      <td>0.103592</td>\n",
       "      <td>0.182030</td>\n",
       "      <td>0.297668</td>\n",
       "      <td>-0.002652</td>\n",
       "      <td>0.086083</td>\n",
       "      <td>-0.020139</td>\n",
       "      <td>0.024291</td>\n",
       "      <td>-0.069403</td>\n",
       "      <td>-0.231655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It was the conclusion we wanted to see - the p...</td>\n",
       "      <td>Dust</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0</td>\n",
       "      <td>17855756</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.535509</td>\n",
       "      <td>[0.2418055, -0.1830729, 0.39519662, -0.0359889...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044703</td>\n",
       "      <td>-0.406086</td>\n",
       "      <td>0.235105</td>\n",
       "      <td>-0.193855</td>\n",
       "      <td>0.113906</td>\n",
       "      <td>-0.282803</td>\n",
       "      <td>0.419765</td>\n",
       "      <td>0.222623</td>\n",
       "      <td>-0.139149</td>\n",
       "      <td>-0.090754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>My problem with this book is there were lots o...</td>\n",
       "      <td>Dust</td>\n",
       "      <td>1</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0</td>\n",
       "      <td>17855756</td>\n",
       "      <td>0.036625</td>\n",
       "      <td>0.024668</td>\n",
       "      <td>0.653747</td>\n",
       "      <td>[0.096865825, 0.07542943, 0.061335944, 0.11228...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320965</td>\n",
       "      <td>-0.098903</td>\n",
       "      <td>0.168757</td>\n",
       "      <td>0.058654</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>-0.019319</td>\n",
       "      <td>-0.196473</td>\n",
       "      <td>0.120615</td>\n",
       "      <td>-0.143962</td>\n",
       "      <td>0.279851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14290</th>\n",
       "      <td>Will read the next.</td>\n",
       "      <td>Forever Odd</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>698</td>\n",
       "      <td>16433</td>\n",
       "      <td>0.177566</td>\n",
       "      <td>0.144543</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[0.10743135, -0.3015935, 0.03974713, 0.0797066...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.358973</td>\n",
       "      <td>-0.126794</td>\n",
       "      <td>0.024562</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.094335</td>\n",
       "      <td>0.336781</td>\n",
       "      <td>0.012558</td>\n",
       "      <td>-0.012272</td>\n",
       "      <td>0.594620</td>\n",
       "      <td>-0.238397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14291</th>\n",
       "      <td>If I only had one word to sum it up then \"odd\".</td>\n",
       "      <td>The Lace Reader</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>699</td>\n",
       "      <td>1951125</td>\n",
       "      <td>0.215878</td>\n",
       "      <td>0.122285</td>\n",
       "      <td>0.631535</td>\n",
       "      <td>[0.12413848, 0.19855367, 0.042200167, 0.019212...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060622</td>\n",
       "      <td>-0.015157</td>\n",
       "      <td>-0.239266</td>\n",
       "      <td>-0.518556</td>\n",
       "      <td>0.096743</td>\n",
       "      <td>-0.104593</td>\n",
       "      <td>0.085093</td>\n",
       "      <td>0.047637</td>\n",
       "      <td>0.067509</td>\n",
       "      <td>0.148426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14292</th>\n",
       "      <td>It did keep swapping from first to third perso...</td>\n",
       "      <td>The Lace Reader</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>699</td>\n",
       "      <td>1951125</td>\n",
       "      <td>0.143460</td>\n",
       "      <td>0.104940</td>\n",
       "      <td>0.356158</td>\n",
       "      <td>[-0.15070404, 0.08648281, 0.079170436, 0.15373...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.363667</td>\n",
       "      <td>0.361313</td>\n",
       "      <td>-0.013605</td>\n",
       "      <td>0.017482</td>\n",
       "      <td>0.059031</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>-0.074386</td>\n",
       "      <td>0.271997</td>\n",
       "      <td>-0.012426</td>\n",
       "      <td>0.021483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14293</th>\n",
       "      <td>Some dark things happen which kind of just get...</td>\n",
       "      <td>The Lace Reader</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>699</td>\n",
       "      <td>1951125</td>\n",
       "      <td>0.119985</td>\n",
       "      <td>0.086142</td>\n",
       "      <td>0.528640</td>\n",
       "      <td>[0.5452099, 0.20342357, 0.3213288, 0.1988398, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.199145</td>\n",
       "      <td>-0.415404</td>\n",
       "      <td>0.013450</td>\n",
       "      <td>-0.126928</td>\n",
       "      <td>0.120654</td>\n",
       "      <td>0.226310</td>\n",
       "      <td>0.314678</td>\n",
       "      <td>0.093227</td>\n",
       "      <td>-0.403474</td>\n",
       "      <td>0.314984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14294</th>\n",
       "      <td>Really odd, and the twist at the end, a little...</td>\n",
       "      <td>The Lace Reader</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>699</td>\n",
       "      <td>1951125</td>\n",
       "      <td>0.215878</td>\n",
       "      <td>0.116832</td>\n",
       "      <td>0.530047</td>\n",
       "      <td>[-0.10798588, -0.28911787, 0.14432462, 0.18672...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053777</td>\n",
       "      <td>-0.305939</td>\n",
       "      <td>-0.016809</td>\n",
       "      <td>-0.384679</td>\n",
       "      <td>0.347311</td>\n",
       "      <td>-0.136916</td>\n",
       "      <td>-0.066333</td>\n",
       "      <td>-0.024981</td>\n",
       "      <td>0.198668</td>\n",
       "      <td>-0.263914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14295 rows Ã— 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences         contexts  \\\n",
       "0                                     What a fun series.             Dust   \n",
       "1      I loved Wool, and Dust and Shift both gave us ...             Dust   \n",
       "2      I think the first book was by far the best, bu...             Dust   \n",
       "3      It was the conclusion we wanted to see - the p...             Dust   \n",
       "4      My problem with this book is there were lots o...             Dust   \n",
       "...                                                  ...              ...   \n",
       "14290                                Will read the next.      Forever Odd   \n",
       "14291    If I only had one word to sum it up then \"odd\".  The Lace Reader   \n",
       "14292  It did keep swapping from first to third perso...  The Lace Reader   \n",
       "14293  Some dark things happen which kind of just get...  The Lace Reader   \n",
       "14294  Really odd, and the twist at the end, a little...  The Lace Reader   \n",
       "\n",
       "       labels  positions  reviewid    bookid  item_spec_score_max  \\\n",
       "0           0   0.000000         0  17855756             0.015290   \n",
       "1           0   0.007143         0  17855756             0.050604   \n",
       "2           0   0.014286         0  17855756             0.050604   \n",
       "3           1   0.021429         0  17855756             0.036474   \n",
       "4           1   0.028571         0  17855756             0.036625   \n",
       "...       ...        ...       ...       ...                  ...   \n",
       "14290       0   0.021429       698     16433             0.177566   \n",
       "14291       0   0.000000       699   1951125             0.215878   \n",
       "14292       1   0.007143       699   1951125             0.143460   \n",
       "14293       0   0.014286       699   1951125             0.119985   \n",
       "14294       0   0.021429       699   1951125             0.215878   \n",
       "\n",
       "       item_spec_score_mean  tfidfscore  \\\n",
       "0                  0.013789    0.785133   \n",
       "1                  0.027685    0.417833   \n",
       "2                  0.026657    0.484192   \n",
       "3                  0.021774    0.535509   \n",
       "4                  0.024668    0.653747   \n",
       "...                     ...         ...   \n",
       "14290              0.144543    1.000000   \n",
       "14291              0.122285    0.631535   \n",
       "14292              0.104940    0.356158   \n",
       "14293              0.086142    0.528640   \n",
       "14294              0.116832    0.530047   \n",
       "\n",
       "                                      combined_embedding  ...       118  \\\n",
       "0      [0.13568804, -0.42419568, 0.22611226, 0.193501...  ... -0.435559   \n",
       "1      [0.27358928, 0.051419273, 0.13299458, 0.160809...  ... -0.457130   \n",
       "2      [-0.07830019, -0.3241384, 0.26186368, -0.03014...  ... -0.182080   \n",
       "3      [0.2418055, -0.1830729, 0.39519662, -0.0359889...  ... -0.044703   \n",
       "4      [0.096865825, 0.07542943, 0.061335944, 0.11228...  ...  0.320965   \n",
       "...                                                  ...  ...       ...   \n",
       "14290  [0.10743135, -0.3015935, 0.03974713, 0.0797066...  ...  0.358973   \n",
       "14291  [0.12413848, 0.19855367, 0.042200167, 0.019212...  ... -0.060622   \n",
       "14292  [-0.15070404, 0.08648281, 0.079170436, 0.15373...  ... -0.363667   \n",
       "14293  [0.5452099, 0.20342357, 0.3213288, 0.1988398, ...  ... -0.199145   \n",
       "14294  [-0.10798588, -0.28911787, 0.14432462, 0.18672...  ...  0.053777   \n",
       "\n",
       "            119       120       121       122       123       124       125  \\\n",
       "0     -0.028487  0.224099  0.136431  0.306240  0.047697  0.344808  0.247321   \n",
       "1     -0.071240  0.153160 -0.042372  0.042692  0.019526  0.195423  0.173676   \n",
       "2      0.103592  0.182030  0.297668 -0.002652  0.086083 -0.020139  0.024291   \n",
       "3     -0.406086  0.235105 -0.193855  0.113906 -0.282803  0.419765  0.222623   \n",
       "4     -0.098903  0.168757  0.058654  0.006225 -0.019319 -0.196473  0.120615   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14290 -0.126794  0.024562  0.004872  0.094335  0.336781  0.012558 -0.012272   \n",
       "14291 -0.015157 -0.239266 -0.518556  0.096743 -0.104593  0.085093  0.047637   \n",
       "14292  0.361313 -0.013605  0.017482  0.059031  0.003847 -0.074386  0.271997   \n",
       "14293 -0.415404  0.013450 -0.126928  0.120654  0.226310  0.314678  0.093227   \n",
       "14294 -0.305939 -0.016809 -0.384679  0.347311 -0.136916 -0.066333 -0.024981   \n",
       "\n",
       "            126       127  \n",
       "0      0.494328 -0.217180  \n",
       "1      0.141578 -0.039212  \n",
       "2     -0.069403 -0.231655  \n",
       "3     -0.139149 -0.090754  \n",
       "4     -0.143962  0.279851  \n",
       "...         ...       ...  \n",
       "14290  0.594620 -0.238397  \n",
       "14291  0.067509  0.148426  \n",
       "14292 -0.012426  0.021483  \n",
       "14293 -0.403474  0.314984  \n",
       "14294  0.198668 -0.263914  \n",
       "\n",
       "[14295 rows x 138 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fac3768d-a199-4dc0-9369-7b12fac51868",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.to_csv('train.csv', index=False)\n",
    "valdf.to_csv('valid.csv', index=False)\n",
    "testdf.to_csv('test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
